# test.yaml
preambles:
  cpu:
    - "#!/bin/bash"
    - "#SBATCH --cpus-per-task=4"
    - "#SBATCH --mem=8G"
    - "#SBATCH --output=slurm/slurm-%j.out"
    - "#SBATCH --error=slurm/slurm-%j.err"

  gpu:
    - "#!/bin/bash"
    - "#SBATCH --cpus-per-task=4"
    - "#SBATCH --mem=8G"
    - "#SBATCH --output=slurm/slurm-%j.out"
    - "#SBATCH --error=slurm/slurm-%j.err"

group:
  name: "ml-pipeline"
  type: sequential
  jobs:
    # Parallel hyperparameter sweep
    - group:
        type: sweep
        sweep:
          lr: [0.001, 0.01, 0.1]
          model: [llama, gemma]
        preamble: gpu
        sweep_template: "sleep 20 && echo 'python train.py --lr {lr} --model {model}'"
    
    # Find best model and run tests
    - job:
        preamble: cpu
        command: "sleep 20 && echo 'python find_best.py --metric eval_loss --group_id {group_id}'"
    
    - job:
        preamble: gpu
        command: "sleep 20 && echo 'python test.py --model best_model.pt --group_id {group_id}'"
    
    - job:
        preamble: cpu
        command: "sleep 20 && echo 'python create_report.py --results test_results.json --group_id {group_id}'"